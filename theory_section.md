# Co-Evolutionary Learning in Automated Mechanism Design: Theoretical Analysis

## 1. Introduction and Framework

We study automated mechanism design (AMD) in first-price sealed-bid auctions where both bidders and the auctioneer learn simultaneously through co-evolutionary dynamics. This setting extends the classical framework of mechanism design [Myerson, 1981; Vickrey, 1961] by allowing the mechanism itself to adapt based on observed outcomes, while bidders simultaneously learn optimal strategies.

### 1.1 Model Specification

Consider $n$ risk-neutral agents with private values $v_i \sim U[0,1]$ independently drawn. Each agent $i$ submits a bid $b_i = v_i \cdot \theta_i$, where $\theta_i \in [0,1]$ represents a shading factor. The auctioneer implements a mechanism $(G, P)$ where:
- **Allocation rule** $G: \mathbb{R}^n \to \Delta^n$ maps bid vectors to allocation probabilities
- **Payment rule** $P: \mathbb{R}^n \times \{1,\ldots,n\} \to \mathbb{R}^n$ determines payments for each agent

Both $G$ and $P$ are parameterized by neural networks and learned through policy gradient methods. Agents learn their bidding strategies $\theta_i$ using either regret matching [Hart and Mas-Colell, 2000] or multi-armed bandit algorithms (UCB, $\epsilon$-greedy).

### 1.2 Co-Evolutionary Dynamics

This co-evolutionary setting fundamentally differs from classical auction theory, which assumes a fixed mechanism and analyzes equilibrium strategies under complete information [Krishna, 2009]. Here, the mechanism itself evolves, creating a non-stationary dynamic game where traditional equilibrium concepts (Nash, Bayesian Nash) may not apply. The auctioneer's objective is to maximize expected revenue, while agents maximize expected utility, creating a strategic interaction that evolves over time.

## 2. Static Equilibrium Predictions vs. Co-Evolutionary Dynamics

### 2.1 Classical First-Price Auction Theory

In a standard first-price sealed-bid auction with $n$ symmetric risk-neutral bidders and independent private values $v_i \sim U[0,1]$, the unique symmetric Bayesian Nash equilibrium (BNE) has each bidder using the strategy:

$$b^*(v) = \frac{n-1}{n} \cdot v = \theta^* \cdot v$$

where $\theta^* = \frac{n-1}{n}$ [Krishna, 2009, Chapter 2]. For $n=10$, this yields $\theta^* = 0.9$. The expected equilibrium revenue is:

$$R^* = \mathbb{E}[\max_i b^*(v_i)] = \frac{n-1}{n+1} \approx 0.818$$

This equilibrium characterization relies on several critical assumptions:
1. **Fixed mechanism**: The auctioneer commits to a predetermined first-price rule
2. **Complete information about the mechanism**: Agents have full knowledge of the payment rule
3. **Stationary environment**: The mechanism remains constant across all periods
4. **Symmetric strategies**: All agents employ identical bidding functions
5. **Common knowledge of rationality**: All agents are rational and this is common knowledge

### 2.2 Co-Evolutionary Learning Dynamics

In our AMD framework, assumptions (1), (3), and (4) are systematically violated, leading to fundamentally different dynamics.

**Proposition 1 (Non-Stationarity)**. Under co-evolutionary learning, the mechanism $(G_t, P_t)$ evolves over time, creating a non-stationary environment. Agents cannot converge to the static BNE strategy $\theta^*$ because the mechanism they face changes continuously.

**Proof Sketch**: The auctioneer updates $(G_t, P_t)$ using policy gradient methods to maximize revenue. Since agent strategies $\{\theta_{i,t}\}$ evolve simultaneously, the optimal mechanism at time $t$ differs from time $t+1$, violating stationarity. $\square$

**Asymmetric Information**: Agents may receive different information signals (e.g., $\mathcal{I}_i \in \{\text{MINIMAL}, \text{WINNER}, \text{LOSER}, \text{FULL\_TRANSPARENCY}, \text{FULL\_REVELATION}\}$), leading to heterogeneous learning dynamics and asymmetric strategies.

**Strategic Interaction**: The auctioneer's revenue maximization objective conflicts with agents' utility maximization, creating a feedback loop:
$$\text{High payments} \rightarrow \text{Lower bids} \rightarrow \text{Lower revenue}$$
$$\text{Low payments} \rightarrow \text{Higher bids but still low revenue}$$

**Empirical Observation**: Our simulations with $n=10$ agents show $\bar{\theta} \approx 0.55$, significantly below the static BNE prediction of $\theta^* = 0.9$. This deviation reflects a co-evolutionary equilibrium where both mechanism and strategies adapt simultaneously.

## 3. The Revenue Paradox: Lower Theta, Variable Revenue

### 3.1 Asymmetric Strategies and Information

A key theoretical insight emerges from the interaction between information revelation and strategy asymmetry. When agents receive heterogeneous information signals, they develop heterogeneous strategies, creating a revenue paradox.

**Proposition 2 (Asymmetric Strategy Formation)**. Under asymmetric information revelation, agents develop heterogeneous bidding strategies:
- Agents with information $\mathcal{I}_i = \text{FULL\_REVELATION}$ may bid aggressively: $\theta_i \in [0.75, 0.80]$
- Agents with limited information maintain near-uniform strategies: $\theta_i \approx 0.5$

**Theorem 1 (Revenue Paradox)**. Despite average shading factor $\bar{\theta} < \theta^*$, revenue $R$ can satisfy $R \geq R^*$ when:
1. At least one agent bids aggressively: $\exists i: \theta_i \geq 0.75$
2. Revenue depends on $\max_i b_i$, not $\bar{b}$
3. The auctioneer's allocation network $G$ learns to favor high bidders

**Proof**: Let $b_i = v_i \theta_i$ and $b^*_i = v_i \theta^*$. If $\max_i \theta_i \geq 0.75$ and values are sufficiently high, then:
$$\max_i b_i = \max_i (v_i \theta_i) \geq \max_i (v_i \cdot 0.75)$$
For high-value realizations, this can exceed $\max_i b^*_i$ when $\bar{\theta} < \theta^*$ but variance in $\{\theta_i\}$ is high. $\square$

This counterintuitive result occurs because:
- **Revenue is determined by the maximum bid, not the mean**: $R = P(\arg\max_i b_i)$
- **Asymmetric information induces asymmetric strategies**: Well-informed agents bid near value, while others shade more
- **The auctioneer's learning mechanism exploits asymmetry**: $G$ allocates to high bidders, while $P$ optimizes pricing

### 3.2 Strategy Concentration and Convergence

In static equilibrium analysis, strategies concentrate at the equilibrium point. However, co-evolutionary learning exhibits fundamentally different behavior.

**Definition 1 (Strategy Concentration)**. For agent $i$ with strategy distribution $p_i(\theta)$ over discrete $\theta$ values, define concentration as:
$$C_i = 1 - \frac{H(p_i)}{H_{\max}}$$
where $H(p_i) = -\sum_\theta p_i(\theta) \log p_i(\theta)$ is entropy and $H_{\max} = \log |\Theta|$ is maximum entropy (uniform distribution).

**Observation 1 (Low Concentration)**. Empirical results show $C_i \in [0.003, 0.016]$ for most agents, indicating near-uniform distributions rather than concentrated strategies.

**Proposition 3 (Non-Convergence to Pure Strategies)**. Under co-evolutionary learning with regret matching, strategies do not concentrate because:
1. **Non-stationarity**: Optimal $\theta$ changes as $(G_t, P_t)$ evolves
2. **Regret matching property**: Maintains probability over all actions with positive cumulative regret
3. **Exploration requirement**: Agents must explore to adapt to changing mechanisms

**Theorem 2 (Expected Theta Paradox)**. Even when $p_i(\theta)$ has mode at $\theta_m \approx 0.78$, the expected value $\mathbb{E}[\theta_i] = \sum_\theta \theta \cdot p_i(\theta) \approx 0.5$ when:
- Probability mass at high $\theta$ is only marginally above uniform: $p_i(\theta_m) \approx 0.003$ vs. uniform $1/|\Theta| = 0.002$
- Remaining $98\%$ of mass is spread uniformly across $\Theta = [0,1]$

**Proof**: For uniform distribution over $|\Theta| = 500$ values, $\mathbb{E}[\theta] = 0.5$. If $p_i(\theta_m) = 0.003$ and remaining mass is uniform, then:
$$\mathbb{E}[\theta_i] = 0.003 \cdot 0.78 + 0.997 \cdot \frac{\sum_{\theta \neq \theta_m} \theta}{499} \approx 0.5$$
$\square$

This explains the empirical observation: agents have "top thetas" around $0.78-0.80$ with probabilities $0.003$, yet expected theta remains $\approx 0.5$.

## 4. Information Revelation and Learning Efficiency

### 4.1 Information Types and Agent Adaptation

We formalize information revelation through an information structure $\mathcal{I} = \{\mathcal{I}_1, \ldots, \mathcal{I}_n\}$ where each $\mathcal{I}_i$ specifies what agent $i$ observes after the auction. We consider:

- **MINIMAL**: $\mathcal{I}_i = \{w_i\}$ where $w_i \in \{0,1\}$ indicates win/loss
- **WINNER**: $\mathcal{I}_i = \{w_i, \{b_j : j \neq i, w_j = 0\}\}$ if $w_i = 1$
- **LOSER**: $\mathcal{I}_i = \{w_i, \max_j b_j\}$ if $w_i = 0$
- **FULL_TRANSPARENCY**: $\mathcal{I}_i = \{b_1, \ldots, b_n\}$ for all $i$
- **FULL_REVELATION**: $\mathcal{I}_i = \{b_1, \ldots, b_n, v_1, \ldots, v_n, p_1, \ldots, p_n\}$ for all $i$

**Proposition 4 (Information and Learning Algorithms)**. The effect of information revelation depends critically on the learning algorithm:

1. **Bandit algorithms** (UCB, $\epsilon$-greedy): Update based solely on utility signals $u_i$. Information revelation has minimal effect since $u_i$ can be computed from $\mathcal{I}_i = \{w_i\}$.

2. **Regret matching**: Requires full bid information to compute counterfactual utilities:
   $$u_i(\theta') = \mathbb{E}[v_i - p_i | b_i = v_i \theta', \mathbf{b}_{-i}]$$
   For regret matching, $\mathcal{I}_i$ must include $\mathbf{b}_{-i}$ to compute regret, necessitating FULL_REVELATION or FULL_TRANSPARENCY.

3. **Co-evolutionary constraint**: Even with full information, strategies may not concentrate (Proposition 3) because the mechanism $(G_t, P_t)$ evolves, requiring continuous adaptation.

### 4.2 Efficiency Loss

Define efficiency as $\eta = \mathbb{P}[\arg\max_i b_i = \arg\max_i v_i]$, the probability that the highest-value agent wins.

**Observation 2 (Efficiency Loss)**. Empirical results show $\eta \approx 0.10-0.15$, far below the theoretical maximum of $\eta^* = 1.0$ for efficient mechanisms.

**Proposition 5 (Efficiency-R revenue Tradeoff)**. Under co-evolutionary learning, the auctioneer's revenue maximization objective may conflict with efficiency:

1. **Allocation network learning**: $G$ learns to allocate to $\arg\max_i b_i$, which may not equal $\arg\max_i v_i$ when shading factors $\{\theta_i\}$ are heterogeneous.

2. **Revenue-efficiency conflict**: Maximizing revenue may favor aggressive bidders regardless of value:
   $$\max_{G,P} \mathbb{E}[R] = \max_{G,P} \mathbb{E}[P(\arg\max_i b_i)]$$
   This need not align with $\arg\max_i v_i$.

3. **Non-stationarity**: As agents adapt $\{\theta_i\}$, the mapping from bids to values changes, making it difficult for $G$ to learn the efficient allocation.

**Corollary 1**. In co-evolutionary AMD, achieving both high revenue and high efficiency simultaneously may be infeasible without explicit multi-objective optimization.

## 5. Alternating Training and Convergence Stability

### 5.1 The Need for Alternating Training

In co-evolutionary learning, simultaneous updates by both agents and auctioneer can lead to instability, preventing convergence.

**Definition 2 (Alternating Training Protocol)**. For training interval $T$:
1. **Phase 1** ($t \in [kT, (k+1)T)$): Agents update strategies $\{\theta_{i,t}\}$ while $(G_t, P_t)$ is frozen
2. **Phase 2** ($t \in [(k+1)T, (k+2)T)$): Auctioneer updates $(G_t, P_t)$ while $\{\theta_{i,t}\}$ are frozen
3. Repeat for $k = 0, 1, 2, \ldots$

**Proposition 6 (Stability via Alternating Training)**. Alternating training creates periods of stability that allow each side to converge toward a best response, approximating a Stackelberg game structure where:
- **Followers** (agents) commit to strategies: $\{\theta_i\} = \arg\max_{\{\theta_i\}} \mathbb{E}[u_i | G, P]$
- **Leader** (auctioneer) best-responds: $(G, P) = \arg\max_{G,P} \mathbb{E}[R | \{\theta_i\}]$

However, unlike true Stackelberg equilibrium [Von Stackelberg, 1934], agents cannot fully commit across phases, leading to ongoing adaptation.

**Theorem 3 (Instability of Simultaneous Updates)**. Under simultaneous updates, the system may not converge to any stable state if learning rates are not carefully tuned.

**Proof Sketch**: Simultaneous updates create a coupled dynamical system:
$$\frac{d\theta_i}{dt} = f_i(\theta_i, G, P), \quad \frac{d(G,P)}{dt} = g(G, P, \{\theta_i\})$$
Without alternating phases, this system may exhibit limit cycles or chaotic behavior. $\square$

### 5.2 Convergence Challenges

Even with alternating training, we observe fundamental convergence challenges.

**Observation 3 (Revenue Degradation)**. Revenue decreases over time: $R_t$ declines from $\sim 0.31$ to $\sim 0.12$, suggesting convergence to a low-revenue co-evolutionary equilibrium.

**Observation 4 (Strategy Dispersion)**. Agents maintain uniform strategies (Proposition 3) rather than concentrating, indicating failure to converge to pure strategies.

**Observation 5 (Efficiency Loss)**. Allocation network $G$ struggles to learn efficient allocation (Proposition 5), with $\eta \approx 0.10-0.15$.

**Conjecture 1 (Co-Evolutionary Equilibrium Properties)**. Co-evolutionary equilibria in AMD may have:
- Lower revenue than static BNE: $R_{co-evo} < R^*$
- Lower efficiency: $\eta_{co-evo} < \eta^*$
- Dispersed strategies: $C_i \approx 0$ for most agents

This suggests fundamental limitations of co-evolutionary learning compared to static mechanism design with known equilibrium strategies.

## 6. Theoretical Implications

### 6.1 Equilibrium Concepts in Co-Evolutionary Settings

Classical auction theory relies on Nash equilibrium concepts (Nash, 1950; Harsanyi, 1967-1968), which assume:
- Fixed mechanism
- Complete information (or common prior)
- Rational agents with common knowledge

In co-evolutionary AMD, these assumptions are systematically violated, necessitating new equilibrium concepts.

**Definition 3 (Co-Evolutionary Equilibrium)**. A state $(\{\theta_i^*\}, G^*, P^*)$ is a co-evolutionary equilibrium if:
1. Given $(G^*, P^*)$, agents cannot improve: $\theta_i^* \in \arg\max_{\theta_i} \mathbb{E}[u_i | \theta_i, \{\theta_{-i}^*\}, G^*, P^*]$
2. Given $\{\theta_i^*\}$, auctioneer cannot improve: $(G^*, P^*) \in \arg\max_{G,P} \mathbb{E}[R | \{\theta_i^*\}, G, P]$
3. The state is stable under the learning dynamics

**Definition 4 (Learning Equilibrium)**. A distribution $\pi(\{\theta_i\}, G, P)$ over strategies and mechanisms that is invariant under the learning dynamics, but may not correspond to a Nash equilibrium.

**Theorem 4 (Equilibrium Properties)**. Co-evolutionary equilibria may have fundamentally different properties than static Nash equilibria:
- Lower revenue: $R_{co-evo} \leq R^*$ (Observation 3)
- Lower efficiency: $\eta_{co-evo} \leq \eta^*$ (Observation 2)
- Dispersed strategies: $C_i \approx 0$ (Observation 1)

**Proof**: Follows from Observations 1-3 and the non-stationarity of the environment (Proposition 1). $\square$

### 6.2 Mechanism Design Implications

The AMD framework raises fundamental questions for mechanism design theory:

**Question 1 (Performance Comparison)**. Can learning mechanisms outperform fixed mechanisms? Our results (Observations 3-5) suggest not necessarilyâ€”co-evolutionary dynamics may converge to worse outcomes than static BNE.

**Question 2 (Optimal Information Revelation)**. What information structure $\mathcal{I}$ maximizes social welfare or revenue? Proposition 4 shows this depends on the learning algorithm: FULL_REVELATION is necessary for regret matching but irrelevant for bandit algorithms.

**Question 3 (Multi-Objective Optimization)**. How to balance revenue and efficiency? Proposition 5 shows these objectives may conflict. A multi-objective approach may be necessary:
$$\max_{G,P} \alpha \cdot \mathbb{E}[R] + (1-\alpha) \cdot \eta$$
for some weight $\alpha \in [0,1]$.

**Question 4 (Training Schedule Optimization)**. What is the optimal alternating training schedule? The choice of $T$ balances stability (larger $T$) vs. adaptability (smaller $T$). This remains an open question.

**Question 5 (Convergence Guarantees)**. Under what conditions does co-evolutionary learning converge? Theorem 3 shows simultaneous updates may not converge, but convergence properties of alternating training remain unproven.

## 7. Conclusion

Co-evolutionary learning in automated mechanism design creates dynamics fundamentally different from classical auction theory. The non-stationary environment (Proposition 1), asymmetric information (Proposition 2), and strategic interaction between learners lead to:

- **Lower strategy concentration** than static equilibria (Proposition 3, Theorem 2)
- **Revenue-efficiency tradeoffs** not present in fixed mechanisms (Proposition 5)
- **Convergence to co-evolutionary equilibria** that may be suboptimal (Theorem 4)
- **Information revelation effects** that depend on the learning algorithm (Proposition 4)

These findings suggest that while AMD offers flexibility and adaptability, it may not automatically outperform well-designed fixed mechanisms. Understanding these dynamics is crucial for practical applications of learning-based mechanism design, and raises fundamental questions about equilibrium concepts, optimal information revelation, and convergence guarantees in co-evolutionary settings.

## References

Harsanyi, J. C. (1967-1968). Games with incomplete information played by "Bayesian" players, I-III. *Management Science*, 14(3), 159-182, 320-334, 486-502.

Hart, S., & Mas-Colell, A. (2000). A simple adaptive procedure leading to correlated equilibrium. *Econometrica*, 68(5), 1127-1150.

Krishna, V. (2009). *Auction Theory*. Academic Press.

Myerson, R. B. (1981). Optimal auction design. *Mathematics of Operations Research*, 6(1), 58-73.

Nash, J. (1950). Equilibrium points in n-person games. *Proceedings of the National Academy of Sciences*, 36(1), 48-49.

Vickrey, W. (1961). Counterspeculation, auctions, and competitive sealed tenders. *The Journal of Finance*, 16(1), 8-37.

Von Stackelberg, H. (1934). *Marktform und Gleichgewicht*. Springer.

